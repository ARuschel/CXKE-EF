{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "interpreter": {
   "hash": "b63b56bfa85f8c46fbc6d538dbc11dde2e6fcd31237f5b4e5862dd2f81efb1e2"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import config\n",
    "from tools.tools import restore_model, get_dirs, write_to_pkl\n",
    "from tools.dataset_tools import Dataset\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import sparse\n",
    "\n",
    "get_ipython().magic(u'load_ext autoreload')\n",
    "get_ipython().magic(u'autoreload 2')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bench_dataset = 'FB15K237' #[FB13, FB15k, FB15k-237, NELL186, WN11, WN18, WN18RR]\n",
    "model = 'TransE'\n",
    "timestamp_emb = '1906141142'\n",
    "splits = 'g_2negrate_bern'\n",
    "\n",
    "project_folder = os.path.expanduser('~') + f'/proj/XKE_results/{bench_dataset}/'\n",
    "emb_folder = project_folder + f'embeddings/{model}/{timestamp_emb}/'\n",
    "splits_folder = project_folder + f'splits/{splits}/'\n",
    "\n",
    "emb_results_folder = project_folder + f'emb_results/{model}_{timestamp_emb}_{splits}/'\n",
    "\n",
    "d = Dataset(bench_dataset)\n",
    "\n",
    "if not os.path.exists(emb_results_folder):\n",
    "    os.makedirs(emb_results_folder)\n",
    "    print('Creating folder: {}.'.format(emb_results_folder))\n",
    "\n",
    "split_statistics = pd.read_csv(splits_folder + 'split_statistics.tsv', sep='\\t', index_col=0)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tf.__version__)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_statistics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "con = config.Config()\n",
    "embd = restore_model(con, emb_folder)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "con.classify_triples([0, 2, 7], [1, 3, 5], [0, 1, 2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embd.classify_triples([0, 2, 7], [1, 3, 5], [0, 1, 2])\n",
    "# should return [True, True, False]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "con.test_step([0, 2, 7], [1, 3, 5], [0, 1, 2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_overall_metrics = pd.DataFrame(columns=['emb:rel_threshold', 'emb:cv_rel_threshold', 'emb:rel_train_acc','emb:cv_rel_train_acc', 'emb:rel_test_acc', 'emb:cv_rel_test_acc', 'emb:rel_train_f1','emb:cv_rel_train_f1', 'emb:rel_test_f1','emb:cv_rel_test_f1', 'emb:rel_train_tp', 'emb:rel_train_fp', 'emb:rel_train_fn', 'emb:rel_train_tn', 'emb:rel_test_tp', 'emb:rel_test_fp', 'emb:rel_test_fn', 'emb:rel_test_tn', 'emb:cv_rel_train_tp', 'emb:cv_rel_train_fp', 'emb:cv_rel_train_fn', 'emb:cv_rel_train_tn', 'emb:cv_rel_test_tp', 'emb:cv_rel_test_fp', 'emb:cv_rel_test_fn', 'emb:cv_rel_test_tn'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_cv_threshold(df):\n",
    "\n",
    "    min_score = df['emb_score'].min()\n",
    "    max_score = df['emb_score'].max()\n",
    "\n",
    "    negative_instances = list(df[df['label'] == 0].index)\n",
    "    K = len(negative_instances) - len(df[df['label'] == 1])\n",
    "    negative_samples = random.sample(negative_instances, k=K)\n",
    "    df.drop(index=negative_samples, inplace=True)\n",
    "\n",
    "    label = df.label.values\n",
    "    emb_score = df.emb_score.values\n",
    "\n",
    "    n_folds = 5\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "    splits = []\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        splits.append(test_index)\n",
    "\n",
    "    thresh_values = np.linspace(min_score, max_score, num=500)\n",
    "\n",
    "    accs = []\n",
    "    for value in thresh_values:\n",
    "        acc = 0\n",
    "        for split in splits:\n",
    "            acc += accuracy_score(emb_score.take(split) < value, label.take(split))\n",
    "        accs.append(acc / n_folds)\n",
    "    \n",
    "    accs = np.array(accs)\n",
    "    cv_thresh = thresh_values[np.argmax(accs)]\n",
    "\n",
    "    return cv_thresh\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "for rel_id in tqdm(split_statistics.index):\n",
    "\n",
    "    rel_name = d.rel_dict[rel_id]\n",
    "    rel_dir = splits_folder + d.rel_dict[rel_id] + '/'\n",
    "    rel_splits = os.listdir(rel_dir)\n",
    "    # rel_id = d.rel_dict_rev[rel]\n",
    "\n",
    "    train_df, valid_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    emb_results_rel_folder = emb_results_folder + rel_name + '/'\n",
    "    if not os.path.exists(emb_results_rel_folder):\n",
    "        os.makedirs(emb_results_rel_folder)\n",
    "\n",
    "    # if len(rel_splits) < 3:\n",
    "    #     continue\n",
    "\n",
    "    thresh = con.relThresh[int(rel_id[1:])]\n",
    "\n",
    "    #train subset\n",
    "    train_df = pd.read_csv(rel_dir + 'train.tsv', sep='\\t')\n",
    "    train_df['rel_thresh'] = thresh\n",
    "    train_df['emb_score'] = con.test_step(train_df.e1.values, train_df.e2.values, train_df.rel.values)\n",
    "\n",
    "    # train_emb_pred = [1 if el == True else 0 for el in train_emb_pred]\n",
    "    train_emb_true = list(train_df.label.values)\n",
    "    train_df['emb_pred'] = train_df['emb_score'] < thresh\n",
    "    train_df['emb_pred'] = train_df['emb_pred'].astype(int)\n",
    "        \n",
    "\n",
    "    #valid subset\n",
    "    if split_statistics.loc[rel_id, 'valid'] > 0:\n",
    "        valid_df = pd.read_csv(rel_dir + 'valid.tsv', sep='\\t')\n",
    "        valid_df['rel_thresh'] = thresh\n",
    "        valid_df['emb_score'] = con.test_step(valid_df.e1.values, valid_df.e2.values, valid_df.rel.values)\n",
    "        valid_df['emb_pred'] = valid_df['emb_score'] < thresh\n",
    "        valid_df['emb_pred'] = valid_df['emb_pred'].astype(int)\n",
    "\n",
    "        train_valid_df = pd.concat([train_df, valid_df])\n",
    "    else:\n",
    "        print('Valid split for rel {} not found, skipping!'.format(rel_name))\n",
    "        train_valid_df = train_df\n",
    "\n",
    "    cv_thresh = get_cv_threshold(train_valid_df)\n",
    "\n",
    "    train_df['emb_cv_thresh'] = cv_thresh\n",
    "    train_df['emb_cv_pred'] = train_df.emb_score.values < cv_thresh\n",
    "    train_df['emb_cv_pred'] = train_df['emb_cv_pred'].astype(int)\n",
    "    train_df.to_csv(emb_results_rel_folder + 'train.tsv', sep='\\t')\n",
    "\n",
    "    if split_statistics.loc[rel_id, 'valid'] > 0:\n",
    "        valid_df['emb_cv_thresh'] = cv_thresh\n",
    "        valid_df['emb_cv_pred'] = valid_df.emb_score.values < cv_thresh\n",
    "        valid_df['emb_cv_pred'] = valid_df['emb_cv_pred'].astype(int)\n",
    "        valid_df.to_csv(emb_results_rel_folder + 'valid.tsv', sep='\\t')\n",
    "\n",
    "    #test subset\n",
    "    if split_statistics.loc[rel_id, 'test'] > 0:\n",
    "        test_df = pd.read_csv(rel_dir + 'test.tsv', sep='\\t')\n",
    "        test_df['rel_thresh'] = thresh\n",
    "        test_df['emb_score'] = con.test_step(test_df.e1.values, test_df.e2.values, test_df.rel.values)\n",
    "        test_df['emb_pred'] = test_df['emb_score'] < thresh\n",
    "        test_df['emb_pred'] = test_df['emb_pred'].astype(int)\n",
    "        test_df['emb_cv_thresh'] = cv_thresh\n",
    "        test_df['emb_cv_pred'] = test_df.emb_score.values < cv_thresh\n",
    "        test_df['emb_cv_pred'] = test_df['emb_cv_pred'].astype(int)\n",
    "        test_df.to_csv(emb_results_rel_folder + 'test.tsv', sep='\\t')\n",
    "\n",
    "    else:\n",
    "        print('Test split for rel {} not found, skipping!'.format(rel_name))\n",
    "\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:rel_threshold'] = thresh\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:cv_rel_threshold'] = cv_thresh\n",
    "\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:rel_train_acc'] = accuracy_score(train_df.label.values, train_df.emb_pred.values)\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:rel_train_f1'] = f1_score(train_df.label.values, train_df.emb_pred.values)\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:rel_train_tn'], emb_overall_metrics.loc[rel_id, 'emb:rel_train_fp'], emb_overall_metrics.loc[rel_id, 'emb:rel_train_fn'], emb_overall_metrics.loc[rel_id, 'emb:rel_train_tp'] =  confusion_matrix(train_df.label.values, train_df.emb_pred.values).ravel()\n",
    "\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:cv_rel_train_acc'] = accuracy_score(train_df.label.values, train_df.emb_cv_pred.values)\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:cv_rel_train_f1'] = f1_score(train_df.label.values, train_df.emb_cv_pred.values)\n",
    "    emb_overall_metrics.loc[rel_id, 'emb:cv_rel_train_tn'], emb_overall_metrics.loc[rel_id, 'emb:cv_rel_train_fp'], emb_overall_metrics.loc[rel_id, 'emb:cv_rel_train_fn'], emb_overall_metrics.loc[rel_id, 'emb:cv_rel_train_tp'] =  confusion_matrix(train_df.label.values, train_df.emb_cv_pred.values).ravel()\n",
    "\n",
    "    if split_statistics.loc[rel_id, 'test'] > 0:\n",
    "        emb_overall_metrics.loc[rel_id, 'emb:rel_test_acc'] = accuracy_score(test_df.label.values, test_df.emb_pred.values)\n",
    "        emb_overall_metrics.loc[rel_id, 'emb:rel_test_f1'] = f1_score(test_df.label.values, test_df.emb_pred.values)\n",
    "        emb_overall_metrics.loc[rel_id, 'emb:rel_test_tn'], emb_overall_metrics.loc[rel_id, 'emb:rel_test_fp'], emb_overall_metrics.loc[rel_id, 'emb:rel_test_fn'], emb_overall_metrics.loc[rel_id, 'emb:rel_test_tp'] =  confusion_matrix(test_df.label.values, test_df.emb_pred.values).ravel()\n",
    "\n",
    "        emb_overall_metrics.loc[rel_id, 'emb:cv_rel_test_acc'] = accuracy_score(test_df.label.values, test_df.emb_cv_pred.values)\n",
    "        emb_overall_metrics.loc[rel_id, 'emb:cv_rel_test_f1'] = f1_score(test_df.label.values, test_df.emb_cv_pred.values)\n",
    "        emb_overall_metrics.loc[rel_id, 'emb:cv_rel_test_tn'], emb_overall_metrics.loc[rel_id, 'emb:cv_rel_test_fp'], emb_overall_metrics.loc[rel_id, 'emb:cv_rel_test_fn'], emb_overall_metrics.loc[rel_id, 'emb:cv_rel_test_tp'] =  confusion_matrix(test_df.label.values, test_df.emb_cv_pred.values).ravel()\n",
    "\n",
    "emb_overall_metrics.fillna(0, inplace=True)\n",
    "emb_overall_metrics.to_csv(emb_results_folder + 'emb_metrics.tsv', sep='\\t')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_overall_metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_acc_emb = (emb_overall_metrics['emb:rel_train_tp'].sum() + emb_overall_metrics['emb:rel_train_tn'].sum()) / (emb_overall_metrics['emb:rel_train_tp'].sum() + emb_overall_metrics['emb:rel_train_tn'].sum() + emb_overall_metrics['emb:rel_train_fp'].sum() + emb_overall_metrics['emb:rel_train_fn'].sum())\n",
    "train_acc_emb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_acc_emb = (emb_overall_metrics['emb:rel_test_tp'].sum() + emb_overall_metrics['emb:rel_test_tn'].sum()) / (emb_overall_metrics['emb:rel_test_tp'].sum() + emb_overall_metrics['emb:rel_test_tn'].sum() + emb_overall_metrics['emb:rel_test_fp'].sum() + emb_overall_metrics['emb:rel_test_fn'].sum())\n",
    "test_acc_emb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_acc_emb_cv = (emb_overall_metrics['emb:cv_rel_train_tp'].sum() + emb_overall_metrics['emb:cv_rel_train_tn'].sum()) / (emb_overall_metrics['emb:cv_rel_train_tp'].sum() + emb_overall_metrics['emb:cv_rel_train_tn'].sum() + emb_overall_metrics['emb:cv_rel_train_fp'].sum() + emb_overall_metrics['emb:cv_rel_train_fn'].sum())\n",
    "train_acc_emb_cv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_acc_emb_cv = (emb_overall_metrics['emb:cv_rel_test_tp'].sum() + emb_overall_metrics['emb:cv_rel_test_tn'].sum()) / (emb_overall_metrics['emb:cv_rel_test_tp'].sum() + emb_overall_metrics['emb:cv_rel_test_tn'].sum() + emb_overall_metrics['emb:cv_rel_test_fp'].sum() + emb_overall_metrics['emb:cv_rel_test_fn'].sum())\n",
    "test_acc_emb_cv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "con.relThresh[82]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_overall_metrics.loc['r82', 'emb:rel_threshold']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build g_hat for this embedding\n",
    "In fact this is not g_hat, it is the set of all "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# in fact this is not g_hat, it is the set of all triples deemed positive by the embedding considering all heads and tails that are within the type constraints of the dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_metrics = pd.read_csv(emb_results_folder + 'emb_metrics.tsv', sep='\\t', index_col=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_metrics.loc['r82', 'emb:rel_threshold']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_metrics.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "direct_rels = [r for r in d.rel_dict.keys() if r[0] != 'i']\n",
    "inverse_rels = [r for r in d.rel_dict.keys() if r[0] == 'i']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type_constraints = d.build_type_constraints()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(type_constraints['r0']['tail_int'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "g_hat = dict()\n",
    "i = 1\n",
    "for rel in tqdm(direct_rels):\n",
    "\n",
    "    # print(f'\\n{i}/{len(direct_rels)}\\tBuilding g_hat for rel {rel}.')\n",
    "    # time.sleep(0.2)\n",
    "\n",
    "    if emb_metrics.loc[rel, 'emb:rel_threshold'] == 0:\n",
    "        threshold = emb_metrics.loc[rel, 'emb:cv_rel_threshold']\n",
    "        # print(f'Using cv_rel_treshold:{threshold} for relation {rel}')\n",
    "    else:\n",
    "        threshold = emb_metrics.loc[rel, 'emb:rel_threshold']\n",
    "\n",
    "    g_hat[rel] = embd.build_emb_rel_matrix(type_constraints[rel]['head_int'], type_constraints[rel]['tail_int'], int(rel[1:]), threshold)\n",
    "    i+=1"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edges = 0\n",
    "\n",
    "for rel in direct_rels:\n",
    "    edges += g_hat[rel].sum()\n",
    "\n",
    "print(f'g_hat has {edges} edges!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Storing g_hat to disk.')\n",
    "with open(emb_results_folder + 'g_hat.pkl', 'wb') as outfile:\n",
    "    pickle.dump(g_hat, outfile, pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}